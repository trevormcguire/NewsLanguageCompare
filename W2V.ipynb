{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab, CBOW, SkipGram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def build_vocab(tokens):\n",
    "    assert type(tokens) is list, \"Tokens must be list\"\n",
    "    if type(tokens[0]) is str: tokens = [tokens] #creates 2d list if a 1d list is passed.\n",
    "    word2int, idx = {\"<pad>\":0}, 1\n",
    "    for sentence in tokens:\n",
    "        for word in sentence:\n",
    "            if word not in word2int:\n",
    "                word2int[word] = idx\n",
    "                idx += 1\n",
    "    int2word = {index: token for token, index in word2int.items()}\n",
    "    return word2int, int2word\n",
    "\n",
    "def generate_cbow(tokens,window_size):\n",
    "    assert window_size % 2 != 0, \"Window Size must be odd number since context words are centered around target word.\"\n",
    "    assert type(tokens) is list, \"Tokens must be 2d list of sentences\"\n",
    "    if type(tokens[0]) in [int,str]: #could be word or int representation of word\n",
    "        tokens = [tokens]\n",
    "    window_size = int((window_size - 1) / 2)\n",
    "    \n",
    "    train_data = []\n",
    "    for sentence in tokens:\n",
    "        for idx in range(2, len(sentence) - 2):\n",
    "            context = [raw_text[idx - 2], \n",
    "                       raw_text[idx - 1],\n",
    "                       raw_text[idx + 1], \n",
    "                       raw_text[idx + 2]]\n",
    "\n",
    "            target = raw_text[idx]\n",
    "            train_data.append((context, target))\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def generate_skipgrams(tokens, window_size, n_neg_samples):\n",
    "    assert window_size % 2 != 0, \"Window Size must be odd number since context words are centered around target word.\"\n",
    "    assert type(tokens) is list, \"Tokens must be 2d list of sentences\"\n",
    "    if type(tokens[0]) in [int,str]: #could be word or int representation of word\n",
    "        tokens = [tokens]\n",
    "    window_size = int((window_size - 1) / 2)\n",
    "    X, y, targets = [], [], []\n",
    "    \n",
    "    vocab_ints = list(vocab.values())\n",
    "    first_sample_size = int(n_neg_samples**2)\n",
    "    \n",
    "    label = np.zeros(n_neg_samples + 1)\n",
    "    label[0] = 1\n",
    "    \n",
    "    for sentence in tokens:\n",
    "        sentence_len = len(sentence)\n",
    "        for idx, word in enumerate(sentence):\n",
    "            window_start, window_end = max(idx - window_size, 0), min(idx + window_size, sentence_len)\n",
    "            context = sentence[window_start : window_end + 1]\n",
    "            neg_sampling = random.sample(vocab_ints, first_sample_size)\n",
    "            neg_sampling = [x for x in neg_sampling if x not in context]#[:n_neg_samples]\n",
    "            context.remove(word)\n",
    "            for c in context:\n",
    "                X.append(word)\n",
    "                y.append( [c] + random.sample(neg_sampling,n_neg_samples) )\n",
    "                #y.append([c]+neg_sampling)\n",
    "                targets.append(label)\n",
    "                \n",
    "    return X, y, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 µs ± 374 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "# generate_skipgrams(token_ints,5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.36 µs ± 95.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "# generate_cbow(token_ints,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "PATH = \"/Users/trevor/DB_BACKUPS/26Apr2021/\"\n",
    "df = pd.read_csv(PATH + \"ArticleData_26Apr21.csv\",encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>status</th>\n",
       "      <th>images</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ABC</th>\n",
       "      <td>2043</td>\n",
       "      <td>2043</td>\n",
       "      <td>2043</td>\n",
       "      <td>2043</td>\n",
       "      <td>2043</td>\n",
       "      <td>2043</td>\n",
       "      <td>2043</td>\n",
       "      <td>2043</td>\n",
       "      <td>2043</td>\n",
       "      <td>445</td>\n",
       "      <td>2043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>3183</td>\n",
       "      <td>3183</td>\n",
       "      <td>3183</td>\n",
       "      <td>3183</td>\n",
       "      <td>3183</td>\n",
       "      <td>3183</td>\n",
       "      <td>3183</td>\n",
       "      <td>3183</td>\n",
       "      <td>3183</td>\n",
       "      <td>616</td>\n",
       "      <td>3183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AlJazeera</th>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>402</td>\n",
       "      <td>1503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBC</th>\n",
       "      <td>2503</td>\n",
       "      <td>2503</td>\n",
       "      <td>2503</td>\n",
       "      <td>2503</td>\n",
       "      <td>2503</td>\n",
       "      <td>2503</td>\n",
       "      <td>2503</td>\n",
       "      <td>2052</td>\n",
       "      <td>2503</td>\n",
       "      <td>372</td>\n",
       "      <td>2503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CBN</th>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>37</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>33</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CBS</th>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>131</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNBC</th>\n",
       "      <td>2696</td>\n",
       "      <td>2696</td>\n",
       "      <td>2696</td>\n",
       "      <td>2696</td>\n",
       "      <td>2696</td>\n",
       "      <td>2683</td>\n",
       "      <td>2696</td>\n",
       "      <td>2693</td>\n",
       "      <td>2696</td>\n",
       "      <td>303</td>\n",
       "      <td>2696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN</th>\n",
       "      <td>5858</td>\n",
       "      <td>5858</td>\n",
       "      <td>5858</td>\n",
       "      <td>5858</td>\n",
       "      <td>5846</td>\n",
       "      <td>4430</td>\n",
       "      <td>5858</td>\n",
       "      <td>3828</td>\n",
       "      <td>5858</td>\n",
       "      <td>731</td>\n",
       "      <td>5858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Forbes</th>\n",
       "      <td>481</td>\n",
       "      <td>481</td>\n",
       "      <td>481</td>\n",
       "      <td>481</td>\n",
       "      <td>481</td>\n",
       "      <td>481</td>\n",
       "      <td>481</td>\n",
       "      <td>481</td>\n",
       "      <td>481</td>\n",
       "      <td>43</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FoxNews</th>\n",
       "      <td>3053</td>\n",
       "      <td>3053</td>\n",
       "      <td>3053</td>\n",
       "      <td>3053</td>\n",
       "      <td>3053</td>\n",
       "      <td>3014</td>\n",
       "      <td>3053</td>\n",
       "      <td>3053</td>\n",
       "      <td>3053</td>\n",
       "      <td>502</td>\n",
       "      <td>3053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NBC</th>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>25</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPR</th>\n",
       "      <td>1210</td>\n",
       "      <td>1210</td>\n",
       "      <td>1210</td>\n",
       "      <td>1210</td>\n",
       "      <td>1210</td>\n",
       "      <td>1210</td>\n",
       "      <td>1210</td>\n",
       "      <td>1210</td>\n",
       "      <td>1210</td>\n",
       "      <td>268</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NYT</th>\n",
       "      <td>5274</td>\n",
       "      <td>5274</td>\n",
       "      <td>5274</td>\n",
       "      <td>5274</td>\n",
       "      <td>5274</td>\n",
       "      <td>4996</td>\n",
       "      <td>5274</td>\n",
       "      <td>5274</td>\n",
       "      <td>5274</td>\n",
       "      <td>846</td>\n",
       "      <td>5274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT</th>\n",
       "      <td>1321</td>\n",
       "      <td>1321</td>\n",
       "      <td>1321</td>\n",
       "      <td>1321</td>\n",
       "      <td>1321</td>\n",
       "      <td>1321</td>\n",
       "      <td>1321</td>\n",
       "      <td>1321</td>\n",
       "      <td>1321</td>\n",
       "      <td>186</td>\n",
       "      <td>1321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reuters</th>\n",
       "      <td>599</td>\n",
       "      <td>599</td>\n",
       "      <td>599</td>\n",
       "      <td>599</td>\n",
       "      <td>599</td>\n",
       "      <td>599</td>\n",
       "      <td>599</td>\n",
       "      <td>475</td>\n",
       "      <td>599</td>\n",
       "      <td>71</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vox</th>\n",
       "      <td>340</td>\n",
       "      <td>340</td>\n",
       "      <td>340</td>\n",
       "      <td>340</td>\n",
       "      <td>340</td>\n",
       "      <td>340</td>\n",
       "      <td>340</td>\n",
       "      <td>340</td>\n",
       "      <td>340</td>\n",
       "      <td>24</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Unnamed: 0    id  authors   url  title  description  date  content  \\\n",
       "source                                                                          \n",
       "ABC              2043  2043     2043  2043   2043         2043  2043     2043   \n",
       "AP               3183  3183     3183  3183   3183         3183  3183     3183   \n",
       "AlJazeera        1503  1503     1503  1503   1503         1503  1503     1503   \n",
       "BBC              2503  2503     2503  2503   2503         2503  2503     2052   \n",
       "CBN               224   224      224   224    224           37   224      224   \n",
       "CBS               326   326      326   326    326          326   326      326   \n",
       "CNBC             2696  2696     2696  2696   2696         2683  2696     2693   \n",
       "CNN              5858  5858     5858  5858   5846         4430  5858     3828   \n",
       "Forbes            481   481      481   481    481          481   481      481   \n",
       "FoxNews          3053  3053     3053  3053   3053         3014  3053     3053   \n",
       "NBC               149   149      149   149    149          149   149      149   \n",
       "NPR              1210  1210     1210  1210   1210         1210  1210     1210   \n",
       "NYT              5274  5274     5274  5274   5274         4996  5274     5274   \n",
       "RT               1321  1321     1321  1321   1321         1321  1321     1321   \n",
       "Reuters           599   599      599   599    599          599   599      475   \n",
       "Vox               340   340      340   340    340          340   340      340   \n",
       "\n",
       "           category  status  images  \n",
       "source                               \n",
       "ABC            2043     445    2043  \n",
       "AP             3183     616    3183  \n",
       "AlJazeera      1503     402    1503  \n",
       "BBC            2503     372    2503  \n",
       "CBN             224      33     224  \n",
       "CBS             326     131     326  \n",
       "CNBC           2696     303    2696  \n",
       "CNN            5858     731    5858  \n",
       "Forbes          481      43     481  \n",
       "FoxNews        3053     502    3053  \n",
       "NBC             149      25     149  \n",
       "NPR            1210     268    1210  \n",
       "NYT            5274     846    5274  \n",
       "RT             1321     186    1321  \n",
       "Reuters         599      71     599  \n",
       "Vox             340      24     340  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"source\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5858"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.source == \"CNN\"].reset_index(drop=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3828"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[df.content.apply(type) != float].reset_index(drop=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3828"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = df.content.to_list()\n",
    "len(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "<class 'list'> <class 'list'> <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#when mining, paragraphs are split by \\n. News paragraphs can be considered sentences...\n",
    "def clean_first(text):\n",
    "    \"\"\"\n",
    "    Get rid of Locations and Source in starting tokens\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"(\\))(\\w)\",r\"\\1 \\2\",text) #add space after parenthesis if missing\n",
    "    text = text.split()\n",
    "    \n",
    "    i = min(7, len(text) - 2)\n",
    "    while i >= 0:\n",
    "        token = text[i]\n",
    "        if re.search(r\"^[-–-——]+\",token) != None:\n",
    "            text = text[i+1:]\n",
    "            break\n",
    "        elif token.find(\")\") != -1:\n",
    "            text = text[i+1:]\n",
    "            break\n",
    "        i -= 1\n",
    "    text = \" \".join(text).strip()\n",
    "    return text\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r\"\\s{2,}\",\" \",text)\n",
    "    text = re.sub(r\"https?\\:[\\/]{2}[^\\s]+\",\"\",text) #urls\n",
    "    text = text.replace(\"&amp;\",\"and\")\n",
    "    text = re.sub(r\"\\([^\\(\\)]+(Photo|Getty Images)[^\\(\\)]{0,}\\)\",\"\",text)\n",
    "    text = re.sub(r\"’|\\u2019\",\"'\",text)\n",
    "    #text = re.sub(r\"([A-Za-z]+)([\\,\\:\\;])\",r\"\\1 \\2\",text)\n",
    "    #text = re.sub(r\"([A-Za-z]{2,})(\\.)\", r\"\\1 \\2\", text)\n",
    "    return text\n",
    "\n",
    "def dequote(text, min_words = 4, tag=True, ret_quotes=False):\n",
    "    \"\"\"\n",
    "    Dequotes text\n",
    "    -----------\n",
    "    Parameters\n",
    "    -----------\n",
    "        min_words --> min number of words in a quote to be considered valid quotation \n",
    "        tag --> replaces quote with a \"<quote>\" tag\n",
    "        ret_quotes --> returns a tuple of dequoted_text, list of quotes\n",
    "    \"\"\"\n",
    "    def _isQuote(match):\n",
    "        match = match.group(0)\n",
    "        if len(match.split()) > min_words:\n",
    "            if tag:\n",
    "                return \"<quote>\"\n",
    "            return \"\"\n",
    "        return match\n",
    "    #***************************************\n",
    "    \n",
    "    text = re.sub(r\"\\u201C|\\u201D\",'\"',text) #normalize fancy unicode quotes\n",
    "\n",
    "    pattern = re.compile(r'((?<=^)|(?<=\\W))\\\"(.+?)\\\"')\n",
    "    dequoted_text = pattern.sub(_isQuote,text).strip()\n",
    "    return dequoted_text\n",
    "\n",
    "#**********************************************************************\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#nlp.disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]\n",
    "\n",
    "def process_entities(text, \n",
    "                     ent_types=[\"ORG\",\"PERSON\",\"GPE\", \"LOCATION\",\"ORGANIZATION\"],\n",
    "                     repl_char=\"_\"):\n",
    "    \"\"\"\n",
    "    Finds entities and transforms into a single token \n",
    "    -------------\n",
    "    Parameters\n",
    "    -------------\n",
    "    ent_types --> types of entities to process\n",
    "    repl_char --> character to replace spaces with in the entity\n",
    "    \"\"\"\n",
    "    def replace_ent(match):\n",
    "        match = match.group(0).strip()\n",
    "        return match.replace(\" \", repl_char)\n",
    "        \n",
    "    def custom_ent_tagger(text):\n",
    "        rules = {\n",
    "            \"<year>\":[\n",
    "                (r\"(?<=\\s[iI]n\\s)\\d{4}\", None), \n",
    "                (r\"(<year>)(\\sand\\s|\\sor\\s|\\,\\s)(\\d{4})\", r\"\\1\\2<year>\")\n",
    "            ], # in <year>, <year> and|,|or <year>\n",
    "            \"<day_of_week>\":[\n",
    "                (r\"([Mm]onday|[Tt](ues|hurs)day|[Ww]ednesday|[Ff]riday|[Ss](at|un)day)\", None)\n",
    "            ],\n",
    "            \"<date>\":[\n",
    "                (r\"(January|February|March|April|May|June|July|August|September|October|November|December)\\s(\\d{1,2})\\,?\\s(\\d{4})\", r\"\\1_\\2_\\3\"),\n",
    "                (r\"(January|February|March|April|May|June|July|August|September|October|November|December)\\s(\\d{1,2})\", None)\n",
    "            ],\n",
    "            \"<money>\":[\n",
    "                (r\"\\$[\\d\\,\\.]+\", None)\n",
    "            ]\n",
    "\n",
    "            \n",
    "        }\n",
    "\n",
    "        for tag, patterns in rules.items():\n",
    "            for query, repl in patterns:\n",
    "                if repl is None:\n",
    "                    text = re.sub(query,tag,text)\n",
    "                else:\n",
    "                    text = re.sub(query,repl,text)\n",
    "\n",
    "        return text\n",
    "        \n",
    "        \n",
    "    for ent in nlp(text).ents:\n",
    "        if ent.label_ in ent_types:   \n",
    "            pattern = ent.text\n",
    "            pattern = re.sub(\"[Tt]he\",\"\",pattern).strip() #drop beginning \"the\"\n",
    "            text = re.sub(re.escape(pattern),replace_ent,text)\n",
    "            \n",
    "        elif ent.label_ == \"DATE\": #get year from spaCy DATE ents\n",
    "            pattern = ent.text\n",
    "            if re.search(r\"(^\\'?\\d\\d$)|(^\\d{4}$)\",pattern) != None:\n",
    "                text = re.sub(pattern,\"<year>\",text)\n",
    "            #month day or year -- may 2020 | may 7\n",
    "            elif re.search(\"([Jj](anuary|uly|une)|[Ff]ebruary|[Mm](arch|ay)|[Aa](pril|ugust)|([Ss]ept|[Nn]ov|[Dd]ec)ember|[Oo]ctober)(\\s\\d{2,})\",pattern) != None:\n",
    "                text = re.sub(pattern,\"<date>\",text)\n",
    "            elif re.search(\"[Mm]ay\",pattern) != None:\n",
    "                text = re.sub(pattern,\"<month_of_may>\",text)\n",
    "\n",
    "\n",
    "    ##seperate paranthesis attached to entity  #national_institute_of_allergy_and_infectious_diseases_(niaid)\n",
    "    text = re.sub(r\"([\\w\\_]+)(\\_)(\\(\\w+\\))\",r\"\\1 \\3\", text) \n",
    "    #get rid of leading _ for ents\n",
    "    text = re.sub(r\"(\\s)(\\_)([\\w])\", r\"\\1 \\3\", text)\n",
    "    #custom rules\n",
    "    text = custom_ent_tagger(text)\n",
    "    #print(text)   \n",
    "    return text\n",
    "\n",
    "\n",
    "def contraction_handling(text):\n",
    "    contractions = {\n",
    "        r\"(\\w+)n't\":r\"\\1 not\",\n",
    "        r\"(\\w+)'ve\":r\"\\1 have\",\n",
    "        r\"(\\w+)'re\":r\"\\1 are\",\n",
    "        r\"(\\w+)'s\":r\"\\1 's\",\n",
    "        r\"(\\w+)'ll\":r\"\\1 will\",\n",
    "        r\"([A-Za-z]+)(\\-)([A-Za-z]+)\":r\"\\1 \\2 \\3\"\n",
    "    }\n",
    "\n",
    "    exceptions = {\n",
    "        \"can't\":\"cannot\",\n",
    "        \"won't\":\"will not\",\n",
    "        \"why'd\":\"why did\",\n",
    "        \"he'd\":\"he would\",\n",
    "        r\"([Ii]t|[Ss]?[Hh]e|[Tt]here)'s\":r\"\\1 is\"\n",
    "    }\n",
    "    \n",
    "    for pattern, repl in exceptions.items():\n",
    "        text = re.sub(pattern,repl,text)\n",
    "    \n",
    "    for pattern, repl in contractions.items():\n",
    "        text = re.sub(pattern,repl,text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punct(tokens):\n",
    "    pattern = re.compile(r\"[\\,\\.\\?\\!\\:\\;\\\"\\-\\)\\(]+|(\\'(?=\\s))\")\n",
    "    if type(tokens) is str:\n",
    "        return re.sub(pattern,\"\",tokens)\n",
    "    else:\n",
    "        return [re.sub(pattern,\"\",t) for t in tokens]\n",
    "\n",
    "def remove_stopwords(tokens, negations=False):\n",
    "    assert type(tokens) is list, \"Tokens must be a list\"\n",
    "    \n",
    "    stop_words = ['the', 'to', 'of', 'and', 'in', 'a', 'that', \"'s\", \"if\",\"them\", \"all\", \"into\", \"-\",\"—\", \"you\",\n",
    "                  'for', 'on', 'is', 'said', 'with', 'it', 'as', 'was', 'he', 'has', \"did\", \"only\", \"still\", \"back\",\n",
    "                  'by', 'are', 'from', 'at', 'have', 'but', 'his', 'be', 'an', 'will', 'who', 'more', \"where\",\n",
    "                  'she', 'also', 'which', 'its', 'they', 'their', 'this', 'about', 'been', \"there\", \"i\", \"any\",\n",
    "                  'had', 'one', 'her', 'than', 'were', 'us', 'or', 'when', \"like\", \"him\", \"just\", \"both\",\n",
    "                  'some', 'after', 'last','told', 'up', \"how\", \"those\", \"while\", \"what\", \"get\", \"then\",\n",
    "                  'out', 'over', 'new', 'during', \"now\", \"so\", \"we\", \"do\", \"these\", \"can\", \"go\", \"my\",\n",
    "                 \"through\", \"though\", \"yours\", \"your\", \"yet\", \"yours\", \"yourself\", \"yourselves\",\n",
    "                  \"whatever\", \"again\", \"am\", \"become\", \"does\", \"doing\", \"each\", \"either\", \"else\", \"even\",\n",
    "                  \"ever\", \"me\", \"keep\", \"hers\", \"off\", \"onto\", \"our\", \"per\", \"side\", \"such\", \"too\", \"via\", \"why\"]\n",
    "\n",
    "    if negations:\n",
    "        stop_words = stop_words + [\"not\", \"no\", \"cannot\"]\n",
    "    \n",
    "    return [t for t in tokens if t not in stop_words]\n",
    "\n",
    "def tokenize(text):\n",
    "    #whitespace\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def remove_unicode(text):\n",
    "    return re.sub(r\"\\\\u[\\d\\b]\",\"\",text)\n",
    "\n",
    "    \n",
    "def clean_document(text):\n",
    "    text = re.sub(\"<br>\",\"\",text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess(sample, remove_stops=True):\n",
    "    sample = clean_document(sample)\n",
    "    paragraphs = [x.strip() for x in sample.split(\"\\n\") if len(x.strip()) > 0]\n",
    "    if paragraphs[0][:14] == \"Fox News Flash\":\n",
    "        paragraphs = paragraphs[1:]\n",
    "    elif paragraphs[0].find(\"FoxCast\") != -1:\n",
    "        paragraphs = paragraphs[1:]\n",
    "        \n",
    "    paragraphs[0] = clean_first(paragraphs[0])\n",
    "    new_paragraphs = []\n",
    "    i =0 \n",
    "    while i <= len(paragraphs) - 2:\n",
    "        curr = paragraphs[i]\n",
    "        if curr[-1].isalnum():\n",
    "            n = paragraphs[i + 1]\n",
    "            n = n.encode(\"ascii\",\"ignore\").decode()\n",
    "            if n[0].islower():\n",
    "                curr = curr + \" \" + n\n",
    "                i += 1\n",
    "        new_paragraphs.append(curr)\n",
    "        i += 1\n",
    "    all_sentences = []\n",
    "    for para in new_paragraphs:\n",
    "        cleaned = clean(para)\n",
    "        cleaned = dequote(cleaned,tag=True)\n",
    "        cleaned = process_entities(cleaned)\n",
    "        cleaned = contraction_handling(cleaned)\n",
    "        cleaned = remove_punct(cleaned)\n",
    "        tokens = tokenize(cleaned.lower())\n",
    "        if remove_stops:\n",
    "            tokens = remove_stopwords(tokens)\n",
    "        all_sentences.append(tokens)\n",
    "    return all_sentences\n",
    "    \n",
    "#***************************************************\n",
    "    \n",
    "documents = []\n",
    "for article in sample[50:55]:\n",
    "    sentences = preprocess(article)\n",
    "    for sentence in sentences:\n",
    "        documents.append(sentence)\n",
    "\n",
    "\n",
    "print(len(documents))\n",
    "print(type(documents),type(documents[0]),type(documents[0][0]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<quote>',\n",
       " 'not',\n",
       " 'arceneaux',\n",
       " 'company',\n",
       " 'space',\n",
       " 'tread',\n",
       " 'americans',\n",
       " 'people',\n",
       " 'home',\n",
       " 'broadband',\n",
       " 'would',\n",
       " 'mission',\n",
       " 'malware',\n",
       " 'researchers',\n",
       " 'st',\n",
       " 'jude',\n",
       " 'take',\n",
       " 'crew',\n",
       " 'peloton',\n",
       " 'publishers',\n",
       " 'fcc',\n",
       " 'united',\n",
       " 'cancer',\n",
       " '4',\n",
       " 'plans',\n",
       " 'many',\n",
       " 'other',\n",
       " 'say',\n",
       " 'internet',\n",
       " 'red',\n",
       " 'use',\n",
       " 'around',\n",
       " 'month',\n",
       " 'inspiration',\n",
       " 'first',\n",
       " 'isaacman',\n",
       " 'could',\n",
       " 'spacex',\n",
       " 'media',\n",
       " 'european',\n",
       " 'irving',\n",
       " 'income',\n",
       " 'according',\n",
       " 'canary',\n",
       " 'silver',\n",
       " 'sparrow',\n",
       " 'found',\n",
       " 'year',\n",
       " 'millions',\n",
       " 'astronauts',\n",
       " 'million',\n",
       " 'make',\n",
       " 'trip',\n",
       " 'including',\n",
       " 'call',\n",
       " 'lower',\n",
       " 'treadmill',\n",
       " 'tread+',\n",
       " 'evancha',\n",
       " 'classes',\n",
       " 'news',\n",
       " 'policy',\n",
       " 'legislation',\n",
       " 'school',\n",
       " 'access',\n",
       " 'low',\n",
       " 'infected',\n",
       " 'used',\n",
       " 'search',\n",
       " 'apple',\n",
       " 'report',\n",
       " 'because',\n",
       " 'states',\n",
       " 'additional',\n",
       " 'members',\n",
       " 'research',\n",
       " 'less',\n",
       " 'whose',\n",
       " 'put',\n",
       " 'no',\n",
       " 'patients',\n",
       " 'nasa',\n",
       " 'work',\n",
       " 'companies',\n",
       " 'designed',\n",
       " 'measures',\n",
       " 'most',\n",
       " 'high',\n",
       " 'december',\n",
       " 'president',\n",
       " 'available',\n",
       " 'pandemic',\n",
       " 'support',\n",
       " 'digital',\n",
       " 'cannot',\n",
       " 'australia',\n",
       " 'tech',\n",
       " 'google',\n",
       " 'week',\n",
       " 'divide']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def keep_n_common(tokens,n):\n",
    "    n_common = [x[0] for x in Counter(tokens).most_common(n)]\n",
    "    return n_common\n",
    "\n",
    "all_tokens = []\n",
    "for sentence in documents:\n",
    "    for word in sentence:\n",
    "        all_tokens.append(word)\n",
    "\n",
    "keep_n_common(all_tokens,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Length: 42312\n"
     ]
    }
   ],
   "source": [
    "vocab, inverse_vocab = build_vocab(documents)\n",
    "all_sentences = [[vocab[x] for x in sentence] for sentence in documents]\n",
    "print(\"Vocab Length: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, context, labels = generate_skipgrams(all_sentences,\n",
    "                                             window_size=5,\n",
    "                                             n_neg_samples=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"abc_training_data.txt\", \"w\") as f:\n",
    "    f.write(str(list(zip(inputs, context, labels))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206316\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (((64,), (64, 5, 1)), (64, 5)), types: ((tf.int64, tf.int64), tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = len(inputs)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((inputs, context), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (((64,), (64, 5, 1)), (64, 5)), types: ((tf.int64, tf.int64), tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(keras.models.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_ns):\n",
    "        super(Word2Vec,self).__init__()\n",
    "        self.target_embedding = keras.layers.Embedding(vocab_size, embedding_dim, input_length = 1, name=\"W2V\")\n",
    "        self.context_embedding = keras.layers.Embedding(vocab_size,embedding_dim,input_length=num_ns+1)\n",
    "        self.dots = keras.layers.Dot(axes=(3, 2))\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        \n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        word_emb = self.target_embedding(target)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        dots = self.dots([context_emb, word_emb])\n",
    "        return self.flatten(dots)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim, num_ns=4)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "34473/34473 [==============================] - 2634s 76ms/step - loss: 0.6535 - accuracy: 0.7849\n",
      "Epoch 2/5\n",
      "34473/34473 [==============================] - 2521s 73ms/step - loss: 0.3696 - accuracy: 0.8847\n",
      "Epoch 3/5\n",
      "34473/34473 [==============================] - 2525s 73ms/step - loss: 0.1898 - accuracy: 0.9444\n",
      "Epoch 4/5\n",
      "34473/34473 [==============================] - 2568s 74ms/step - loss: 0.0937 - accuracy: 0.9739\n",
      "Epoch 5/5\n",
      "34473/34473 [==============================] - 2527s 73ms/step - loss: 0.0475 - accuracy: 0.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc34ccd6210>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, string\n",
    "\n",
    "weights = word2vec.get_layer('W2V').get_weights()[0]\n",
    "\n",
    "out_v = io.open('abc_vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('abc_vocab.tsv', 'w', encoding='utf-8')\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
